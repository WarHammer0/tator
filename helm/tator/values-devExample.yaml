# This can be a domain that points to a LAN IP address if you
# are accessing the site from another machine's browser or you
# are installing Tator in a VM. Using localhost only works in 
# a browser running on the same machine running Tator.
domain: &domain localhost
# Requests for these domains will be redirected to the actual domain.
#redirects:
#  - domain: mysite.duckdns.org
#    # Key filename that will be used in secret for this domain.
#    tlsKeyFile: mysite_key.key
#    # Cert filename that will be used in secret for this domain.
#    tlsCertFile: mysite_cert.crt
#    # Key secret name that will be used in secret for this domain.
#    tlsKeySecretName: mysite-tls-key
#    # Cert secret name that will be used in secret for this domain.
#    tlsCertSecretName: mysite-tls-cert
# Enables debug mode for gunicorn. Do NOT enable this in production.
tatorDebug: true
# Enables javascript minification.
useMinJs: false
# Enable this to turn on "down for maintenance" page.
maintenance: false
# Secret key for django. Feel free to change this.
djangoSecretKey: "9q@$1)+x+zh-3csau(zqhheos2e+ncygac#ol2)1@x2w#kkaer"
postgresHost: "postgis-svc"
# Postgres username. Some make commands expect username django, but otherwise
# it can be changed.
postgresUsername: "django"
# Postgres password. Change this for production.
postgresPassword: "django123"
redisHost: "tator-redis-master"
elasticsearchHost: "elasticsearch-master"
objectStorageHost: "minio-master"
# If you are using the docker registry container for your registry, you can
# leave these, otherwise change user/pass to the credentials for your registry.
dockerUsername: "test"
dockerPassword: "test"
dockerRegistry: "localhost:5000"
systemImageRepo: "localhost:5000"
#slackToken: "<Your slack API token>" # Optional, for slack notifications
#slackChannel: "<Your slack channel ID>" # Optional, for slack notifications
# Enable this to require HTTPS. Be sure to set true for production deployments!
requireHttps: false
certCron:
  # Enable this to enable a cron job to automatically update certificates
  # periodically from LetsEncrypt. If this is not provided, the Secret objects
  # tls-cert and tls-key must be created manually.
  enabled: false
# List of storage classes for use by workflows. One of these will be randomly
# passed as a workflow parameter to algorithm workflows, and randomly selected
# for transcode workflows.
workflowStorageClasses:
  - nfs-client
pv:
  nfsServer: "127.0.0.1"
  nfsMountOptions:
    - nfsvers=4.1
  path: "/media/kubernetes_share"
  # Specifies a series of persistent volumes for storing media. If given,
  # the main pv is no longer used for storing new media, and instead each
  # media file is written to a randomly selected shard from those given.
  # Media files stored on a shard use the name given to construct a URI.
  # This is shown here for an example, but typically in dev environments
  # there should be no shards.
  #mediaShards:
  #  - name: media000
  #    nfsServer: "127.0.0.1"
  #    nfsMountOptions:
  #      - nfsvers=4.1
  #    path: "/media/kubernetes_share/media000"
  #  - name: media001
  #    nfsServer: "127.0.0.1"
  #    nfsMountOptions:
  #      - nfsvers=4.1
  #    path: "/media/kubernetes_share/media001"
  #  - name: media002
  #    nfsServer: "127.0.0.1"
  #    nfsMountOptions:
  #      - nfsvers=4.1
  #    path: "/media/kubernetes_share/media002"
  # Specifies a series of persistent volumes for storing uploads. If given,
  # the main pv is no longer used for storing uploads, and instead each
  # upload will be routed to one of the shards given below by hashing the
  # upload UID. One TUS service will be created for each shard.
  # This is shown here for an example, but typically in dev environments
  # there should be no shards.
  #uploadShards:
  #  - name: upload000
  #    nfsServer: "127.0.0.1"
  #    nfsMountOptions:
  #      - nfsvers=4.1
  #    path: "/media/kubernetes_share/upload000"
  #  - name: upload001
  #    nfsServer: "127.0.0.1"
  #    nfsMountOptions:
  #      - nfsvers=4.1
  #    path: "/media/kubernetes_share/upload001"
  #  - name: upload002
  #    nfsServer: "127.0.0.1"
  #    nfsMountOptions:
  #      - nfsvers=4.1
  #    path: "/media/kubernetes_share/upload002"
pvc:
  size: 10Ti
hpa:
  nginxMinReplicas: 1
  nginxMaxReplicas: 10
  nginxCpuPercent: 50
  gunicornMinReplicas: 1
  gunicornMaxReplicas: 10
  gunicornCpuPercent: 50
metallb:
  # Enable this to provide a load balancer implementation on bare metal.
  enabled: true
  existingConfigMap: metallb-config
  # Change these to your LAN IP if you are accessing Tator via a browser on
  # another machine or you are running Tator in a VM.
  ipRangeStart: 127.0.0.1
  ipRangeStop: 127.0.0.1
  loadBalancerIp: 127.0.0.1
postgis:
  # Enable this if you want to use the postgis docker image.
  enabled: true
  persistence:
    size: 10Gi
  hostPath: /media/kubernetes_share/postgis
redis:
  # Enable this to install the redis helm chart.
  enabled: true
  master:
    persistence:
      enabled: false
  slave:
    persistence:
      enabled: false
  nodeSelector:
    dbServer: "yes"
  usePassword: false
metrics-server:
  enabled: true
  args:
    - --v=2
    - --kubelet-insecure-tls=true
    - --kubelet-preferred-address-types=InternalIP
elasticsearch:
  # Enable this to install the elasticsearch helm chart.
  enabled: true
  persistence:
    enabled: true
  replicas: 1
  clusterHealthCheckParams: wait_for_status=yellow&timeout=1s
  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    resources:
      requests:
        storage: 30Gi
  nodeSelector: 
    dbServer: "yes"
  hostPath: /media/kubernetes_share/elasticsearch
filebeat:
  enabled: true
  image: docker.elastic.co/beats/filebeat-oss
  imageTag: 7.4.2
  filebeatConfig:
    filebeat.yml: |
      filebeat.inputs:
      - type: docker
        containers.ids:
        - '*'
        processors:
        - add_kubernetes_metadata:
            in_cluster: true
      output.elasticsearch:
        hosts: '${ELASTICSEARCH_HOSTS:elasticsearch-master:9200}'
      setup.ilm.enabled: false
kibana:
  enabled: true
  kibanaConfig:
    kibana.yml: |
      server:
        basePath: /logs
minio:
  enabled: true
  accessKey: "AKIAIOSFODNN7EXAMPLE"
  secretKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
  persistence:
    size: "1Ti"
    existingClaim: "main-pv-claim"
    subPath: "objects"
  defaultBucket:
    enabled: true
    name: tator
kube-prometheus-stack:
  enabled: true
  prometheus:
    server:
      extraArgs:
        web.external-url: /prometheus/
        web.route-prefix: "/"
    prometheusSpec:
      additionalScrapeConfigs:
        - job_name: nginx_status
          scrape_interval: 10s
          metrics_path: "/metrics"
          static_configs:
            - targets: ["nginx-svc:9113"]
  grafana:
    grafana.ini:
      server:
        domain: *domain
        root_url: "%(protocol)s://%(domain)s/grafana/"
        serve_from_sub_path: true
      auth.anonymous:
        enabled: true
        org_role: Admin
prometheus-adapter:
  prometheus:
    url: http://prometheus-operated
    port: 9090
    path: ""
  rules:
    default: true
    custom:
    - seriesQuery: 'nginx_http_requests_total'
      resources:
        overrides:
          instance: {resource: "node"}
          namespace: {resource: "namespace"}
          pod: {resource: "pod"}
      name:
        matches: "^(.*)_total"
        as: "${1}_per_second"
      metricsQuery: "sum(rate(nginx_http_requests_per_second[1m])) by (<<.GroupBy>>)"
# Limits on resources for transcode workflows.
transcoderCpuLimit: "1000m"
transcoderMemoryLimit: "4Gi"
remoteTranscodes:
  # Typically for dev processing is done on the same machine.
  enabled: false
  # Host/port are obtained via the following (run on the transcode cluster):
  #   echo $(kubectl config view --minify | grep server | cut -f 2- -d ":" | tr -d " ")
  host: "your.transcode.domain.org"
  port: "6443"
  # Token can be obtained via the following (run on the transcode cluster):
  #   SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ')
  #   TOKEN=$(kubectl describe secret $SECRET_NAME | grep -E '^token' | cut -f2 -d':' | tr -d " ")
  #   echo $TOKEN
  token: "Bearer <Your token here>"
  # Certificate can be obtained via the following (run on the transcode cluster):
  #   SECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ')
  #   CERT=$(kubectl get secret $SECRET_NAME -o yaml | grep -E '^  ca.crt' | cut -f2 -d':' | tr -d " ")
  #   echo $CERT | base64 --decode
  cert: |
    -----BEGIN CERTIFICATE-----
    <Insert certificate here>
    -----END CERTIFICATE-----
cognito:
  enabled: false
  config: |
    aws-region: us-east-2
    pool-id: <POOL ID HERE>
    client-id: <ID HERE>
